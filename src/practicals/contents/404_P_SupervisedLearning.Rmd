```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = Sys.getenv("GRANOLARR_HOME"))
rm(list = ls())
```

# Supervised machine learning

*[Stefano De Sabbata](https://stefanodesabbata.com)*

[This work](https://github.com/sdesabbata/granolarr) is licensed under the [GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.html). Contains public sector information licensed under the [Open Government Licence v3.0](http://www.nationalarchives.gov.uk/doc/open-government-licence).

## Introduction

The field of **machine learning** sits at the intersection of computer science and statistics, and it is a core component of data science. According to Mitchell (1997), *"the field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience."*

Machine learning approaches are divided into two main types.

- **Supervised**:
    - training of a *"predictive"* model from data;
    - one (or more) attribute of the dataset is used to "predict" another attribute.

- **Unsupervised**:
    - discovery of *descriptive* patterns in data;
    - commonly used in data mining.

Classification is one of the classic supervised machine learning tasks, where algorithms are used to learn (i.e., model) the relationship between a series of input values (a.k.a. predictors, independent variables) and output categorical values or labels (a.k.a. outcome, dependent variable). A model trained on a training dataset can learn the relationship between the input and the labels, and then be used to label new, unlabeled data.



### Artificial netural networks

Artificial neural networks (ANNs) are one of the most studied approaches in supervised machine learning, and the term actually defines a large set of different approaches. These model aims to simulate a simplistic version of a brain made of artificial neuranos. Each artificial neuron combines a series of input values into one output values, using a series of weights (one per input value) and an activation function. The aim of the model is to learn an optimal set of weights that, once cobined with the input values, generates the correct output value. The latter is also influenced by the activation function, which modulates the final result.

![](images/artificial_neuron.png)

Each neuron is effectively a regression model. The input values are the predictors (or independent variables), the output is the outcome (or dependent variable) and the weights are the coefficients (see also previous practical on regression models). The selection of the activation function defines the regression model. As ANNs are commonly used for classification, one of the most common activation functions used are sigmoids, thus rendering each single neuron a logistic regression.

An instance of an ANN is defined by its topology (number of layers and nodes), activation functions and the algorithm used to train the network. The selection of all those parameters reders the construction of ANNs a very complex task, and the quality of the result frequently relies on the experience of the data scientist.

- Number of layers
  - Single-layer network: one node of input variable one node per category of output variable, effectively a logistic regression.
  - Multi-layer network: adds one hidden layer, which aims to caputre hidden *"features"* of the data, as combinations of the input values, and use that for the final classification.
  - Deep neural networks: several hidden layers, each aiming to capture more and more complex *"features"* of the data.
- Number of nodes
  - The number of nodes needs to be selected for each one of the hidden layers.



### Support vector machines

Support vector machines (SVMs) are anothe very common approach to supervised classification. SVMs perform the classification task by partitioning the data space into regions separated by hyperplanes. For instance, in a bi-dimensional space, a hyperplane is a line, and the algorithm is designed to find the line that best separates two groups of data. Computationally, the process is not dissimilar to a linear regression.



## Examples


The two examples below explore the relation between some of the variables from the United Kingdom 2011 Census included among the 167 initial variables used to create the [2011 Output Area Classification](https://maps.cdrc.ac.uk/#/geodemographics/oac11/default/BTTTFFT/12/-1.1233/52.6454/) ([Gale *et al.*, 2016](http://josis.net/index.php/josis/article/view/232/150)) and the [Rural Urban Classification (2011) of Output Areas in England and Wales](https://geoportal.statistics.gov.uk/datasets/rural-urban-classification-2011-of-output-areas-in-england-and-wales) created by the [Office for National Statistics](https://geoportal.statistics.gov.uk/). The various examples and models explore whether it is possible to learn the rural-urban distinction by using some of those census variables, in the Local Authority Districts (LADs) in Leicestershire (excluding the city of Leicester itself.

### Data

The examples use the same data seen in previous practicals, but for the 7 LADs in Leicestershire outside the boundaries of the city of Leicester: Blaby, Charnwood, Harborough, Hinckley and Bosworth, Melton, North West Leicestershire, and Oadby and Wigston. Those data are loaded from the `2011_OAC_Raw_uVariables_Leicestershire.csv`. The second part of the code extracts the data of the Rural Urban Classification (2011) from the compressed file `RUC11_OA11_EW.zip`, loads the extracted data and finally delets them.

```{r, echo=TRUE, eval=FALSE}
# # Libraries
# library(tidyverse)
# library(magrittr)
# 
# # 2011 OAC data for Leicestershire (excl. Leicester)
# liec_shire_2011OAC <- readr::read_csv("2011_OAC_Raw_uVariables_Leicestershire.csv")
# 
# # Rural Urban Classification (2011)
# unzip("RUC11_OA11_EW.zip")
# ru_class_2011 <- readr::read_csv("RUC11_OA11_EW.csv")
# unlink("RUC11_OA11_EW.csv")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Libraries
library(tidyverse)
library(magrittr)

liec_shire_2011OAC <- readr::read_csv(paste0(Sys.getenv("GRANOLARR_HOME"), "/data/", "2011_OAC_Raw_uVariables_Leicestershire.csv"))

unzip(paste0(Sys.getenv("GRANOLARR_HOME"), "/data/", "RUC11_OA11_EW.zip"), exdir = paste0(Sys.getenv("GRANOLARR_HOME"), "/data"))
ru_class_2011 <- readr::read_csv(paste0(Sys.getenv("GRANOLARR_HOME"), "/data/", "RUC11_OA11_EW.csv"))
unlink(paste0(Sys.getenv("GRANOLARR_HOME"), "/data/", "RUC11_OA11_EW.csv"))
```

We can then join the two datasets and create a simplified, binary rural - urban classification, that is used in the examples below.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
liec_shire_2011OAC_RU <-
  liec_shire_2011OAC %>%
  dplyr::left_join(ru_class_2011) %>%
  dplyr::mutate(
    rural_urban = 
      forcats::fct_recode(
        RUC11CD,
        urban = "C1",
        rural = "D1",
        rural = "E1",
        rural = "F1"
      ) %>% 
      forcats::fct_relevel(
        c("rural", "urban")
      )
  )
```


### Logistic regression

Can we predict whether an Output Area (OA) is urban or rural, solely based on its population density? 

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 6, fig.height = 3}
liec_shire_2011OAC_RU %>%
  ggplot2::ggplot(
    aes(
      x = u006, 
      y = rural_urban
    )
  ) +
  ggplot2::geom_point(
    aes(
      color = rural_urban,
      shape = rural_urban
    )
  ) +
  ggplot2::scale_color_manual(values = c("deepskyblue2", "darkgreen")) +
  ggplot2::scale_x_log10() +
  ggplot2::theme_bw()
```

The two patters in the plot above seem quite close even when plotted using a logarithmically transformed x-axis. As a first step, we can extract from the dataset only the data we need, and create a logarithmic trasformation of the population density vlaue. In oder to be able to perform a simplecross-validatin our model, we can divide that data in a training (80% of the dataset) and a testing set (20% of the dataset).

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Data for logit model
ru_logit_data <-
  liec_shire_2011OAC_RU %>%
  dplyr::select(OA11CD, u006, rural_urban) %>%
  dplyr::mutate(
    density_log = log10(u006)
  )

# Training set
ru_logit_data_trainig <-
  ru_logit_data %>% 
  slice_sample(prop = 0.8)

# Testing set
ru_logit_data_testing <- 
  ru_logit_data %>% 
  anti_join(ru_logit_data_trainig)
```

We can then compute the logit model using the `stats::glm` function and specifying `binomial()` as `family`. The summary of the model highlights how the model is significant, but `Residual deviance` is fairly close to the `Null deviance` (null model), which is not a good sign.

```{r, echo=TRUE}
ru_logit_model <- 
  ru_logit_data_trainig %$%
  stats::glm(
    rural_urban ~ 
      density_log, 
    family = binomial()
  )

ru_logit_model %>%  
  summary()
```

As per other regression models, it would be necessary to test the assumptions of the logit model and the overall distribution of the residuals. However, as this model only has one predictor, we will confine our performance analysis to a cross-validation. 

We can test the performance of the model through a cross-validation exercise, using the testing dataset. Fianlly, we can compare the restults of the prediction with the original data using a confusion matrix.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ru_logit_prediction <- 
  ru_logit_model %>%
  # Use model to predict values
  stats::predict(
    ru_logit_data_testing, 
    type = "response"
  ) %>%
  as.numeric()

ru_logit_data_testing <- 
  ru_logit_data_testing %>%
  tibble::add_column(
    # Add column with predicted class
    logit_predicted_ru = 
      # Values below 0.5 indicate first factor level (rural)
      # Values above 0.5 indicate second factor level (ruban)
      ifelse(
        ru_logit_prediction <= 0.5,
        "rural", # first factor level
        "urban"  # second factor level
      ) %>%
      forcats::as_factor() %>% 
      forcats::fct_relevel(
        c("rural", "urban")
      )
  )

# Load library for confusion matrix
library(class)

# Confusion matrix
caret::confusionMatrix(
  ru_logit_data_testing %>% dplyr::pull(logit_predicted_ru),
  ru_logit_data_testing %>% dplyr::pull(rural_urban),
  mode = "prec_recall"
)
```

### Support vector machines

To showcase the use of SVMs, let's try build a model for urban - rural classification that uses total population and area (logarithmically transformed) as two separate input values. The aim of the SVM is then to find a line that maximises the margin between the two groups shown in the plot below.

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 6, fig.height = 3}
liec_shire_2011OAC_RU %>%
  ggplot2::ggplot(
    aes(
      x = u006, 
      y = Total_Population
    )
  ) +
  ggplot2::geom_point(
    aes(
      color = rural_urban,
      shape = rural_urban
    )
  ) +
  ggplot2::scale_color_manual(values = c("deepskyblue2", "darkgreen")) +
  ggplot2::scale_x_log10() +
  ggplot2::scale_y_log10() +
  ggplot2::theme_bw()
```

The plot illustrates how the two variables are skewed (note that the axis are logarithmically transformed) and that the two groups are not linearly separable. We can thus follow a procedure similar to the one seen above: extract the necessary data; split the data between training and testing for cross-validation; build the model; predict the values for the testing set and interpret the confusion matrix.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Data for SVM model
ru_svm_data <-
  liec_shire_2011OAC_RU %>%
  dplyr::select(OA11CD, Total_Population, u006, rural_urban) %>%
  dplyr::mutate(
    area_log = log10(u006),
    population_log = log10(Total_Population)
  )

# Training set
ru_svm_data_trainig <-
  ru_svm_data %>% 
  slice_sample(prop = 0.8)

# Testing set
ru_svm_data_testing <- 
  ru_svm_data %>% 
  anti_join(ru_svm_data_trainig)

# Load library for svm function
library(e1071)

# Build the model
ru_svm_model <- 
  ru_svm_data_trainig %$%
  e1071::svm(
    rural_urban ~ 
      area_log + population_log, 
    # Use a simple linear hyperplane
    kernel = "linear", 
    # Scale the data
    scale = TRUE,
    # Cost value for observations
    # crossing the hyperplane
    cost = 10
  )

# Predict the values for the testing dataset
ru_svm_prediction <-
  stats::predict(
    ru_svm_model,
    ru_svm_data_testing %>% 
      dplyr::select(area_log, population_log)
  )

# Add predicted values to the table
ru_svm_data_testing <-
  ru_svm_data_testing %>%
  tibble::add_column(
    svm_predicted_ru = ru_svm_prediction
  )

# Confusion matrix
caret::confusionMatrix(
  ru_svm_data_testing %>% dplyr::pull(svm_predicted_ru),
  ru_svm_data_testing %>% dplyr::pull(rural_urban),
  mode = "prec_recall"
  )
```

Let's try something more complex, building a model for rural - urban classification using the presence of different dwelling types as input variables.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Data for SVM model
ru_dwellings_data <-
  liec_shire_2011OAC_RU %>%
  dplyr::select(
    OA11CD, rural_urban, Total_Dwellings,
    u086:u090
  ) %>%
  # scale across
  dplyr::mutate(
    dplyr::across( 
      u086:u090,
      scale
      #function(x){ (x / Total_Dwellings) * 100 }
    )
  ) %>%
  dplyr::rename(
    scaled_detached = u086,
    scaled_semidetached = u087,
    scaled_terraced = u088,
    scaled_flats = u089,	
    scaled_carava_tmp = u090
  ) %>%
  dplyr::select(-Total_Dwellings)

# Training set
ru_dwellings_data_trainig <-
  ru_dwellings_data %>% 
  slice_sample(prop = 0.8)

# Testing set
ru_dwellings_data_testing <- 
  ru_dwellings_data %>% 
  anti_join(ru_dwellings_data_trainig)

# Build the model
ru_dwellings_svm_model <- 
  ru_dwellings_data_trainig %$%
  e1071::svm(
    rural_urban ~ 
      scaled_detached + scaled_semidetached + scaled_terraced + 
      scaled_flats + scaled_carava_tmp, 
    # Use a simple linear hyperplane
    kernel = "linear",
    # Cost value for observations
    # crossing the hyperplane
    cost = 10
  )

# Predict the values for the testing dataset
ru_dwellings_svm_prediction <-
  stats::predict(
    ru_dwellings_svm_model,
    ru_dwellings_data_testing %>% 
      dplyr::select(scaled_detached:scaled_carava_tmp)
  )

# Add predicted values to the table
ru_dwellings_data_testing <-
  ru_dwellings_data_testing %>%
  tibble::add_column(
    dwellings_svm_predicted_ru = ru_dwellings_svm_prediction
  )

# Confusion matrix
caret::confusionMatrix(
  ru_dwellings_data_testing %>% dplyr::pull(dwellings_svm_predicted_ru),
  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),
  mode = "prec_recall"
  )
```

Clearly the linear model is not adequate.

### SVM kernels

Instead of replying on simple linear hperplanes, we can use the *"kernel trick"* to project the data into a higher-dimensional space, which might allow groups to be (more easily) linearly separable. 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Build a second model
# using a radial kernel
ru_dwellings_svm_radial_model <- 
  ru_dwellings_data_trainig %$%
  e1071::svm(
    rural_urban ~ 
      scaled_detached + scaled_semidetached + scaled_terraced + 
      scaled_flats + scaled_carava_tmp, 
    # Use a radial kernel
    kernel = "radial",
    # Cost value for observations
    # crossing the hyperplane
    cost = 10
  )

# Predict the values for the testing dataset
ru_svm_dwellings_radial_prediction <-
  stats::predict(
    ru_dwellings_svm_radial_model,
    ru_dwellings_data_testing %>% 
      dplyr::select(scaled_detached:scaled_carava_tmp)
  )

# Add predicted values to the table
ru_dwellings_data_testing <-
  ru_dwellings_data_testing %>%
  tibble::add_column(
    dwellings_radial_predicted_ru = ru_svm_dwellings_radial_prediction
  )

# Confusion matrix
caret::confusionMatrix(
  ru_dwellings_data_testing %>% dplyr::pull(dwellings_radial_predicted_ru),
  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),
  mode = "prec_recall"
  )
```

```{r}
# library(ROCR)
# library(pROC)
# 
# ru_svm_dwellings_radial_prob <-
#   stats::predict(
#     ru_dwellings_svm_radial_model,
#     ru_dwellings_data_testing %>% 
#       dplyr::select(scaled_detached:scaled_carava_tmp),
#     type="prob", probability = TRUE
#   )
# 
# ru_svm_dwellings_radial_rocr <-
#   prediction(
#   attr(ru_svm_dwellings_radial_prob, "probabilities")[,1], 
#   oa_for_testing %>% dplyr::pull(rural_urban)
# )
# 
# ru_svm_dwellings_radial_performance <- 
#   performance(ru_svm_dwellings_radial_rocr, "tpr","fpr")
# 
# plot(ru_svm_dwellings_radial_performance)
```

### Artificial neural network

Let's try build an ANNs using the same input and output variable.

```{r, echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
# Run this code separately 
# to compute and save the model

# Load library for ANNs
library(neuralnet)

# Build a third model
# using an ANN
ru_dwellings_nnet_model <-
  neuralnet::neuralnet(
    rural_urban ~ 
      scaled_detached + scaled_semidetached + scaled_terraced + 
      scaled_flats + scaled_carava_tmp,
    data = ru_dwellings_data_trainig,
    # Use 2 hidden layers
    hidden = c(5, 2),
    # Max num of steps for training
    stepmax = 1000000
  )

ru_dwellings_nnet_model %>%
  saveRDS(paste0(Sys.getenv("GRANOLARR_HOME"), "/data/", "ru_dwellings_nnet_model.rds"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load the model computed using the code above
# from data folder
ru_dwellings_nnet_model <-
  readRDS(paste0(Sys.getenv("GRANOLARR_HOME"), "/data/", "ru_dwellings_nnet_model_5_2_001.rds"))
```

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Load library for ANNs
library(neuralnet)

# Build a third model
# using an ANN
ru_dwellings_nnet_model <-
  neuralnet::neuralnet(
    rural_urban ~ 
      scaled_detached + scaled_semidetached + scaled_terraced + 
      scaled_flats + scaled_carava_tmp,
    data = ru_dwellings_data_trainig,
    # Use 2 hidden layers
    hidden = c(5, 2),
    # Max num of steps for training
    stepmax = 1000000
  )
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
ru_dwellings_nnet_model %>%  plot(rep = "best")

# Predict the values for the testing dataset
ru_dwellings_nnet_prediction <-
  neuralnet::compute(
    ru_dwellings_nnet_model,
    ru_dwellings_data_testing %>% 
      dplyr::select(scaled_detached:scaled_carava_tmp)
  )

# Add predicted values to the table
ru_dwellings_data_testing <-
  ru_dwellings_data_testing %>%
  tibble::add_column(
    dwellings_nnet_predicted_ru = 
      ru_dwellings_nnet_prediction %$%
      net.result %>%
      max.col %>%
      recode(
        `1` = "rural",
        `2` = "urban"
      ) %>%
      forcats::as_factor() %>% 
      forcats::fct_relevel(
        c("rural", "urban")
      )
  )

# Confusion matrix
caret::confusionMatrix(
  ru_dwellings_data_testing %>% dplyr::pull(dwellings_nnet_predicted_ru),
  ru_dwellings_data_testing %>% dplyr::pull(rural_urban),
  mode = "prec_recall"
  )
```


```{r cleanup, include=FALSE}
rm(list = ls())
```